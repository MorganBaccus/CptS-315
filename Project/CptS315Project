{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CptS315Project","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"m08dbufmNxQV","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1619477518410,"user_tz":420,"elapsed":1068611,"user":{"displayName":"Morgan Baccus","photoUrl":"","userId":"00562493714977998132"}},"outputId":"9aead960-0094-4135-ac9a-1ce7d5bb50c3"},"source":["import numpy as np\n","import pandas as pd\n","import csv\n","\n","#import matplotlib.pyplot as plt\n","import os\n","import re\n","import shutil\n","import string\n","import tensorflow as tf\n","\n","from keras.models import Model, Sequential\n","from keras.layers import Dense, Embedding, Input,  Activation\n","from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras import initializers, optimizers, layers\n","from sklearn.metrics import roc_auc_score\n","\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","from nltk.tokenize import word_tokenize \n","print(stopwords.words('english'))\n","import re                                  # library for regular expression operations\n","import string                              # for string operations\n","from nltk.stem import PorterStemmer        # module for stemming\n","from nltk.tokenize import regexp_tokenize   # module for tokenizing strings\n","from nltk.tokenize import TreebankWordTokenizer\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","\n","#importing the dataset\n","train=pd.read_csv('/content/gdrive/My Drive/CptS315Project/train.csv')\n","test=pd.read_csv('/content/gdrive/My Drive/CptS315Project/test.csv')\n","# view a small selection\n","print(train.head(10))\n","\n","#Global parameters\n","exclude_stop_words = True\n","stopWords = stopwords.words('english')\n","\n","# define functions to clean the text\n","def cleanWords(text):\n","    text = text.lower()\n","    text = re.sub('\\[.*?\\]', '', text)\n","    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n","    text = re.sub('<.*?>+', '', text)\n","    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n","    text = re.sub('\\n', '', text)\n","    text = re.sub('\\w*\\d\\w*', '', text)\n","    return text\n","\n","def removeStopWords(text): \n","   tokenizer = TreebankWordTokenizer()\n","   comment_tokens = tokenizer.tokenize(text)\n","\n","   newText = [word for word in comment_tokens if word not in stopWords]\n","   return newText\n"," \n","def process(comment):\n","   \"\"\"Process  function.\n","   Input:\n","      comment: a string containing a comment\n","   Output:\n","      comments_clean: a list of words containing the processed comment\n","   \"\"\"\n","   stemmer = PorterStemmer()\n","   stopwords_english = stopwords.words('english')\n","   # remove stock market tickers like $GE\n","   comment = re.sub(r'\\$\\w*', '', comment)\n","   # remove old style text \"RT\"\n","   comment = re.sub(r'^RT[\\s]+', '', comment)\n","   # remove hyperlinks\n","   comment = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', comment)\n","   # remove hashtags\n","   # only removing the hash # sign from the word\n","   comment = re.sub(r'#', '', comment)\n","   # tokenize comments\n","   tokenizer = TreebankWordTokenizer()\n","   comment_tokens = tokenizer.tokenize(comment)\n","\n","   comments_clean = []\n","   for word in comment_tokens:\n","      if (word not in stopwords_english and  # remove stopwords\n","               word not in string.punctuation):  # remove punctuation\n","            # tweets_clean.append(word)\n","            stem_word = stemmer.stem(word)  # stemming word\n","            # possibly only add if the word is not ''\n","            comments_clean.append(stem_word)\n","\n","   return comments_clean\n","\n","# take out all puntcuation\n","train['comment_text'] = train['comment_text'].apply(lambda x: cleanWords(x))\n","print(train.head(10))\n","test['comment_text'] = test['comment_text'].apply(lambda x: cleanWords(x))\n","\n","# clean stop words from train and test\n","train['comment_text'] = train['comment_text'].apply(lambda x: removeStopWords(x))\n","print(train.head(10))\n","test['comment_text'] = test['comment_text'].apply(lambda x: removeStopWords(x))\n","print(test.head(10))\n","\n","# copied from kaggle\n","cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n","targets = train[cols].values\n","\n","train_df = train['comment_text']\n","test_df = test['comment_text']\n","\n","max_features = 22000\n","tokenizer = Tokenizer(num_words = max_features)\n","tokenizer.fit_on_texts(list(train_df))\n","\n","tokenized_train = tokenizer.texts_to_sequences(train_df)\n","tokenized_test = tokenizer.texts_to_sequences(test_df)\n","\n","maxlen = 200\n","X_train = pad_sequences(tokenized_train, maxlen = maxlen)\n","X_test = pad_sequences(tokenized_test, maxlen = maxlen)\n","\n","embed_size = 128\n","maxlen = 200\n","max_features = 22000\n","\n","inp = Input(shape = (maxlen, ))\n","x = Embedding(max_features, embed_size)(inp)\n","x = LSTM(60, return_sequences=True, name='lstm_layer')(x)\n","x = GlobalMaxPool1D()(x)\n","x = Dropout(0.1)(x)\n","x = Dense(50, activation=\"relu\")(x)\n","x = Dropout(0.1)(x)\n","x = Dense(6, activation=\"sigmoid\")(x)\n","\n","model = Model(inputs=inp, outputs=x)\n","model.compile(\n","    loss='binary_crossentropy',\n","    optimizer='adam',\n","    metrics=['accuracy'])\n","\n","# print summary of model\n","print(model.summary())\n","\n","batch_size = 64\n","epochs = 2\n","print(model.fit(X_train, targets, batch_size=batch_size, epochs=epochs, validation_split=0.1))\n","\n","\n","\"\"\" \n","nrow_train=train.shape[0]\n","nrow_test=test.shape[0]\n","sum=nrow_train+nrow_test\n","print(\"       : train : test\")\n","print(\"rows   :\",nrow_train,\":\",nrow_test)\n","print(\"perc   :\",round(nrow_train*100/sum),\"   :\",round(nrow_test*100/sum)) \"\"\"\n","\n","# sort vocab alphabetically\n","#vocab.sort()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n","Mounted at /content/gdrive\n","                 id  ... identity_hate\n","0  0000997932d777bf  ...             0\n","1  000103f0d9cfb60f  ...             0\n","2  000113f07ec002fd  ...             0\n","3  0001b41b1c6bb37e  ...             0\n","4  0001d958c54c6e35  ...             0\n","5  00025465d4725e87  ...             0\n","6  0002bcb3da6cb337  ...             0\n","7  00031b1e95af7921  ...             0\n","8  00037261f536c51d  ...             0\n","9  00040093b2687caa  ...             0\n","\n","[10 rows x 8 columns]\n","                 id  ... identity_hate\n","0  0000997932d777bf  ...             0\n","1  000103f0d9cfb60f  ...             0\n","2  000113f07ec002fd  ...             0\n","3  0001b41b1c6bb37e  ...             0\n","4  0001d958c54c6e35  ...             0\n","5  00025465d4725e87  ...             0\n","6  0002bcb3da6cb337  ...             0\n","7  00031b1e95af7921  ...             0\n","8  00037261f536c51d  ...             0\n","9  00040093b2687caa  ...             0\n","\n","[10 rows x 8 columns]\n","                 id  ... identity_hate\n","0  0000997932d777bf  ...             0\n","1  000103f0d9cfb60f  ...             0\n","2  000113f07ec002fd  ...             0\n","3  0001b41b1c6bb37e  ...             0\n","4  0001d958c54c6e35  ...             0\n","5  00025465d4725e87  ...             0\n","6  0002bcb3da6cb337  ...             0\n","7  00031b1e95af7921  ...             0\n","8  00037261f536c51d  ...             0\n","9  00040093b2687caa  ...             0\n","\n","[10 rows x 8 columns]\n","                 id                                       comment_text\n","0  00001cee341fdb12  [yo, bitch, ja, rule, succesful, youll, ever, ...\n","1  0000247867823ef7                            [rfc, title, fine, imo]\n","2  00013b17ad220c46                [sources, zawe, ashton, lapland, â€”]\n","3  00017563c3f7919a  [look, back, source, information, updated, cor...\n","4  00017695ad8997eb                [dont, anonymously, edit, articles]\n","5  0001ea8717f6de06  [thank, understanding, think, highly, would, r...\n","6  00024115d4cbde0f  [please, add, nonsense, wikipedia, edits, cons...\n","7  000247e83dcc1211                        [dear, god, site, horrible]\n","8  00025358d4737918  [fool, believe, numbers, correct, number, lies...\n","9  00026d1092fe71cc  [double, redirects, fixing, double, redirects,...\n","Model: \"model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         [(None, 200)]             0         \n","_________________________________________________________________\n","embedding (Embedding)        (None, 200, 128)          2816000   \n","_________________________________________________________________\n","lstm_layer (LSTM)            (None, 200, 60)           45360     \n","_________________________________________________________________\n","global_max_pooling1d (Global (None, 60)                0         \n","_________________________________________________________________\n","dropout (Dropout)            (None, 60)                0         \n","_________________________________________________________________\n","dense (Dense)                (None, 50)                3050      \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 50)                0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 6)                 306       \n","=================================================================\n","Total params: 2,864,716\n","Trainable params: 2,864,716\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/2\n","2244/2244 [==============================] - 453s 201ms/step - loss: 0.1389 - accuracy: 0.8030 - val_loss: 0.0527 - val_accuracy: 0.9940\n","Epoch 2/2\n","2244/2244 [==============================] - 442s 197ms/step - loss: 0.0488 - accuracy: 0.9684 - val_loss: 0.0509 - val_accuracy: 0.9937\n","<tensorflow.python.keras.callbacks.History object at 0x7fd77a37f4d0>\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["' \\nnrow_train=train.shape[0]\\nnrow_test=test.shape[0]\\nsum=nrow_train+nrow_test\\nprint(\"       : train : test\")\\nprint(\"rows   :\",nrow_train,\":\",nrow_test)\\nprint(\"perc   :\",round(nrow_train*100/sum),\"   :\",round(nrow_test*100/sum)) '"]},"metadata":{"tags":[]},"execution_count":2}]}]}